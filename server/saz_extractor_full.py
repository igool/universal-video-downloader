import zipfile
import re
import os
import shlex
import subprocess
import requests
from urllib.parse import urlparse


# ================================================================
# å›¾ç‰‡è¯†åˆ«æ­£åˆ™
# ================================================================
IMAGE_PATTERNS = [
    r"https?://[^\"'\s]+\.(?:jpg|jpeg|png|gif|webp|bmp|svg)(?:\?[^\"'\s]*)?",
    r"/[^\"'\s]+\.(?:jpg|jpeg|png|gif|webp|bmp|svg)(?:\?[^\"'\s]*)?",
]


# ================================================================
# SAZ è§£æï¼šæå–è¯·æ±‚ & å“åº”ï¼ˆåŒ…å« headersï¼‰
# ================================================================
def parse_saz(saz_path):
    requests_map = {}
    responses_map = {}

    with zipfile.ZipFile(saz_path, "r") as z:
        namelist = z.namelist()

        req_files = [f for f in namelist if f.endswith("_c.txt")]
        resp_files = [f for f in namelist if f.endswith("_s.txt")]

        # ------------ è§£æè¯·æ±‚(_c.txt) ------------
        for rf in req_files:
            rid = rf.split("_")[0]
            raw = z.read(rf).decode("utf-8", "ignore")
            lines = raw.splitlines()
            if not lines:
                continue

            m = re.match(r"(GET|POST|HEAD|OPTIONS)\s+(\S+)\s+HTTP", lines[0])
            if not m:
                continue

            method = m.group(1)
            url = m.group(2)
            headers = {}

            for line in lines[1:]:
                if ":" in line:
                    k, v = line.split(":", 1)
                    headers[k.strip()] = v.strip()

            requests_map[rid] = {
                "url": url,
                "method": method,
                "headers": headers
            }

        # ------------ è§£æå“åº”(_s.txt) ------------
        for sf in resp_files:
            rid = sf.split("_")[0]
            raw = z.read(sf).decode("utf-8", "ignore")

            header_block = raw.split("\r\n\r\n")[0]
            header_lines = header_block.splitlines()[1:]

            headers = {}
            for line in header_lines:
                if ":" in line:
                    k, v = line.split(":", 1)
                    headers[k.strip()] = v.strip()

            content_type = headers.get("Content-Type", "").lower()

            responses_map[rid] = {
                "headers": headers,
                "content_type": content_type
            }

    return requests_map, responses_map


# ================================================================
# æŸ¥æ‰¾ m3u8 è§†é¢‘ï¼ˆåŸºäº Content-Typeï¼‰
# ================================================================
def find_hls_entries(requests_map, responses_map):
    hls_list = []

    for rid, resp in responses_map.items():
        ct = resp["content_type"]
        if (ct.startswith("application/vnd.apple.mpegurl")
                or ct.startswith("application/x-mpegurl")):
            if rid in requests_map:
                hls_list.append(requests_map[rid])

    return hls_list


# ================================================================
# è¯†åˆ«å›¾ç‰‡ URL
# ================================================================
def extract_image_urls(saz_path):
    urls = set()

    with zipfile.ZipFile(saz_path, "r") as z:
        for name in z.namelist():
            if not (name.endswith(".txt") or name.endswith(".xml")):
                continue

            try:
                raw = z.read(name).decode("utf-8", "ignore")
            except:
                continue

            for pat in IMAGE_PATTERNS:
                for u in re.findall(pat, raw, flags=re.IGNORECASE):
                    urls.add(u)

    return list(urls)


# ================================================================
# æ„é€ å®Œæ•´ URLï¼ˆå¤„ç† /abc/xx.png è¿™ç§ç›¸å¯¹è·¯å¾„ï¼‰
# ================================================================
def build_full_url(url, headers):
    if url.startswith("http"):
        return url

    host = headers.get("Host")
    if host:
        return f"http://{host}{url}"

    return url


# ================================================================
# ä¸‹è½½å›¾ç‰‡ï¼ˆå¸¦ headerï¼‰
# ================================================================
def download_image(url, headers, save_dir):
    os.makedirs(save_dir, exist_ok=True)

    filename = urlparse(url).path.split("/")[-1] or "image.bin"
    save_path = os.path.join(save_dir, filename)

    print(f"ğŸ–¼  å›¾ç‰‡ä¸‹è½½: {url}")

    try:
        resp = requests.get(url, headers=headers, timeout=10)
        if resp.status_code == 200:
            with open(save_path, "wb") as f:
                f.write(resp.content)
            print(f"âœ” ä¿å­˜æˆåŠŸ: {save_path}")
        else:
            print(f"âŒ ä¸‹è½½å¤±è´¥ {resp.status_code}: {url}")
    except Exception as e:
        print(f"âŒ è¯·æ±‚é”™è¯¯: {url} --> {e}")

    return save_path


# ================================================================
# æ„å»º ffmpeg å‘½ä»¤ï¼ˆå¸¦å®Œæ•´ headerï¼‰
# ================================================================
def build_ffmpeg_cmd(url, headers, output_path):
    header_args = []

    for k, v in headers.items():
        header_args.append("-headers")
        header_args.append(f"{k}: {v}")

    cmd = [
        "ffmpeg",
        "-y",
        *header_args,
        "-i", url,
        "-c", "copy",
        output_path
    ]

    return cmd


# ================================================================
# ä¸»æµç¨‹ï¼ˆè§†é¢‘ + å›¾ç‰‡ï¼‰
# ================================================================
def extract_from_saz(saz_path, output_dir="./output"):
    os.makedirs(output_dir, exist_ok=True)

    print(f"ğŸ“¦ è§£æ SAZ æ–‡ä»¶ï¼š{saz_path}")

    requests_map, responses_map = parse_saz(saz_path)

    # -------------------- è§†é¢‘æå– --------------------
    print("\nğŸ” æœç´¢è§†é¢‘æµ (m3u8)...")
    hls_entries = find_hls_entries(requests_map, responses_map)

    video_outputs = []

    if hls_entries:
        print(f"âœ” æ‰¾åˆ° {len(hls_entries)} ä¸ªè§†é¢‘æµ")
    else:
        print("âš  æœªæ‰¾åˆ°è§†é¢‘æµ")

    for idx, entry in enumerate(hls_entries, 1):
        url = entry["url"]
        headers = entry["headers"]

        out_path = os.path.join(output_dir, f"video_{idx}.mp4")

        print("\n------------------------------------")
        print(f"ğŸ¬ å¯¼å‡ºè§†é¢‘ {idx}")
        print("------------------------------------")

        cmd = build_ffmpeg_cmd(url, headers, out_path)

        print("\næ‰§è¡Œ ffmpeg å‘½ä»¤ï¼š")
        print(" ".join(shlex.quote(c) for c in cmd))

        subprocess.run(cmd)
        print(f"ğŸ‰ è§†é¢‘å·²ä¿å­˜ï¼š{out_path}")

        video_outputs.append(out_path)

    # -------------------- å›¾ç‰‡æå– --------------------
    print("\nğŸ” æ­£åœ¨æå–å›¾ç‰‡ URL...")
    img_urls = extract_image_urls(saz_path)
    print(f"âœ” æ‰¾åˆ° {len(img_urls)} å¼ å›¾ç‰‡")

    image_save_dir = os.path.join(output_dir, "images")
    os.makedirs(image_save_dir, exist_ok=True)

    for img_url in img_urls:
        # æ‰¾å›¾ç‰‡å±äºå“ªä¸ªè¯·æ±‚ï¼ˆéœ€è¦åŒ¹é… headerï¼‰
        matched_header = None

        for rid, req in requests_map.items():
            # ç²¾ç¡®åŒ¹é…æˆ–å‰ç¼€åŒ¹é…
            if req["url"] == img_url or img_url.startswith(req["url"]):
                matched_header = req["headers"]
                break

        if not matched_header:
            continue

        full_url = build_full_url(img_url, matched_header)

        download_image(full_url, matched_header, image_save_dir)

    print("\n=====================================")
    print(" ğŸ‰ æ‰€æœ‰èµ„æºå·²æå–å®Œæˆï¼")
    print("=====================================")
    print("ğŸ“½ è§†é¢‘è¾“å‡ºï¼š")
    for v in video_outputs:
        print("  âœ”", v)
    print("\nğŸ–¼ å›¾ç‰‡è¾“å‡ºï¼š")
    print(f"  âœ” {image_save_dir}")
    print("=====================================")


# ================================================================
# å¯åŠ¨å…¥å£
# ================================================================
if __name__ == "__main__":
    import sys

    if len(sys.argv) < 2:
        print("ç”¨æ³•ï¼š python saz_extractor_full.py your.saz [output_dir]")
        exit()

    saz_path = sys.argv[1]
    output_dir = sys.argv[2] if len(sys.argv) > 2 else "./output"

    extract_from_saz(saz_path, output_dir)
